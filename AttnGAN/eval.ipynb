{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttnGAN evaluation notebook\n",
    "\n",
    "- quantitative analysis\n",
    "    - inception score\n",
    "    - frechet inception distance\n",
    "    - R-precision\n",
    "- Interactive image generation from caption\n",
    "    - from caption in the dataset\n",
    "    - from caption of your own\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Models\n",
    "    - Generator\n",
    "    - Text Encoder\n",
    "    - (Image Encoder)\n",
    "- Dataset\n",
    "    - COCO\n",
    "- Calculation Resource\n",
    "    - 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setup\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.device_count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "data_dir = '../../data/COCO'\n",
    "exp_dir = 'results/AttnGAN/COCO/2019_05_14_17_08'\n",
    "G_epoch = 50\n",
    "batch_size = 100\n",
    "test_num = batch_size * 30\n",
    "\n",
    "# load config from txt\n",
    "cfg_list_str = ['image_encoder_path', 'text_encoder_path']\n",
    "cfg_list_int = ['words_num', 't_dim', 'z_dim', 'c_dim', 'ngf', 'branch_num', 'base_size']\n",
    "cfg_list_float = []\n",
    "cfg = {}\n",
    "\n",
    "print('experimental setting')\n",
    "with open(os.path.join(exp_dir, 'config.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        print(line[:-1])\n",
    "        split_line = line.split(' ')\n",
    "        key = split_line[0][:-1]\n",
    "        if key in cfg_list_str:\n",
    "            cfg[key] = split_line[1][:-1]\n",
    "        if key in cfg_list_int:\n",
    "            cfg[key] = int(split_line[1])\n",
    "        if key in cfg_list_float:\n",
    "            cfg[key] = float(split_line[1])\n",
    "\n",
    "assert len(cfg.keys()) == len(cfg_list_str) + len(cfg_list_int) + len(cfg_list_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "imsize = 299\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "dataset = TextDataset(data_dir, 'val2014', base_size=imsize, branch_num=1, words_num=cfg['words_num'], transform=image_transform)\n",
    "# dataset = TextDataset(data_dir, 'train2014', base_size=imsize, branch_num=1, words_num=cfg['words_num'], transform=image_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last=True, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model preparation\n",
    "from models import CNNEncoder\n",
    "from models import RNNEncoder\n",
    "from models import AttentionalGNet\n",
    "    \n",
    "image_encoder = CNNEncoder(cfg['t_dim'], download=False).to(device)\n",
    "image_encoder.load_state_dict(torch.load(cfg['image_encoder_path']))\n",
    "image_encoder.eval()\n",
    "\n",
    "text_encoder = RNNEncoder(dataset.n_words, cfg['words_num'], nhidden=cfg['t_dim']).to(device)\n",
    "text_encoder.load_state_dict(torch.load(cfg['text_encoder_path']))\n",
    "text_encoder.eval()\n",
    "\n",
    "G = AttentionalGNet(cfg['z_dim'], cfg['t_dim'], cfg['c_dim'], cfg['ngf'], device, cfg['branch_num']).to(device)\n",
    "G.load_state_dict(torch.load(os.path.join(exp_dir, 'model', 'G_epoch%d.pth' % (G_epoch))))\n",
    "G.eval()\n",
    "print('model load complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create images for scores\n",
    "gen_imsize = cfg['base_size'] * (2 ** (cfg['branch_num'] - 1))\n",
    "\n",
    "noise = torch.FloatTensor(batch_size, cfg['z_dim']).to(device)\n",
    "caps = np.empty((test_num, cfg['words_num']))\n",
    "real_imgs = np.empty((test_num, 3, imsize, imsize))\n",
    "imgs = np.empty((test_num, 3, gen_imsize, gen_imsize))\n",
    "start, end = 0, 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader):\n",
    "        if i >= test_num // batch_size:\n",
    "            break\n",
    "        start = end\n",
    "        end = start + batch_size\n",
    "\n",
    "        noise.data.normal_()\n",
    "        real_images, captions, cap_lens, _, _ = prepare_data(data, device)\n",
    "        real_imgs[start:end] = real_images[-1].detach().cpu().numpy()\n",
    "        caps[start:end] = captions.detach().cpu().numpy()\n",
    "        words_embs, sent_emb = text_encoder(captions, cap_lens)\n",
    "        mask = (captions == 0)\n",
    "        num_words = words_embs.size(2)\n",
    "        if mask.size(1) > num_words:\n",
    "            mask = mask[:, :num_words]\n",
    "        fake_imgs, _, _, _ = G(noise, sent_emb.detach(), words_embs.detach(), mask)\n",
    "        \n",
    "        imgs[start:end] = fake_imgs[-1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inception score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.inception_score.inception_score import inception_score\n",
    "\n",
    "inception_value, _ = inception_score(imgs, device)\n",
    "print('inception score: %.3f' % (inception_value))\n",
    "\n",
    "# dset_inception_value, _ = inception_score(real_imgs, device)\n",
    "# print('inception score of the dataset: %.3f' % (dset_inception_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frechet inception distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.fid.fid_score import fid\n",
    "\n",
    "fid_value = fid(device).calculate_score(real_imgs, imgs)\n",
    "print('fid score: %.3f' % (fid_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.R_precision.r_precision import r_precision\n",
    "\n",
    "r_precision_value = r_precision(dataset, G, cfg['z_dim'], image_encoder, text_encoder, device, 100)\n",
    "print('R-precision: %.3f' % (r_precision_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save eval result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(exp_dir, 'eval_result%d.txt' % (G_epoch)), 'w') as f:\n",
    "    print('inception_score: %.3f' % (inception_value))\n",
    "    f.write('inception_score: %.3f\\n' % (inception_value))\n",
    "    print('fid: %.3f' % (fid_value))\n",
    "    f.write('fid: %.3f\\n' % (fid_value))\n",
    "    print('R-precision: %.3f\\n' % (r_precision_value))\n",
    "    f.write('R-precision: %.3f\\n' % (r_precision_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "def show_tensor(tensor):\n",
    "    image = tensor.add_(1).div_(2).cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def show_np_arr(arr):\n",
    "    plt.imshow(arr)\n",
    "    \n",
    "def caption_convert(caption):\n",
    "    cap = caption.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(cap.lower())\n",
    "    caption_new = []\n",
    "    for t in tokens:\n",
    "        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "        if len(t) > 0:\n",
    "            caption_new.append(dataset.wordtoix[t])\n",
    "    cap_len = len(caption_new)\n",
    "    assert cap_len <= cfg['words_num']\n",
    "    return np.asarray(caption_new).astype('int64'), cap_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with Caption in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "data = next(data_iter)\n",
    "_, captions, cap_lens, _, _ = prepare_data(data, device)\n",
    "noise = torch.FloatTensor(batch_size, cfg['z_dim']).normal_().to(device)\n",
    "with torch.no_grad():\n",
    "    words_embs, sent_emb = text_encoder(captions, cap_lens)\n",
    "    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "    mask = (captions == 0)\n",
    "    num_words = words_embs.size(2)\n",
    "    if mask.size(1) > num_words:\n",
    "        mask = mask[:, :num_words]\n",
    "\n",
    "    fake_imgs, attention_maps, _, _ = G(noise, sent_emb, words_embs, mask)\n",
    "    img = fake_imgs[len(attention_maps)].detach().cpu()\n",
    "    lr_img = fake_imgs[len(attention_maps) - 1].detach().cpu()\n",
    "    attn_maps = attention_maps[-1]\n",
    "    att_sze = attn_maps.size(2)\n",
    "    \n",
    "img_set, _ = utils.build_super_images(img, captions, dataset.ixtoword, attn_maps, att_sze, batch_size, cfg['words_num'], lr_imgs=lr_img)\n",
    "\n",
    "plt.figure(figsize=(10,10), dpi=500)\n",
    "show_np_arr(img_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Image Generation with your Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = 'a cat is lying on the desk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap, cap_len = caption_convert(caption)\n",
    "cap = torch.from_numpy(cap).unsqueeze(0).to(device)\n",
    "cap_len = torch.tensor(cap_len).unsqueeze(0).to(device)\n",
    "noise = torch.FloatTensor(1, cfg['z_dim']).data.normal_().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    words_embs, sent_emb = text_encoder(cap, cap_len)\n",
    "    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "    mask = (cap == 0)\n",
    "    num_words = words_embs.size(2)\n",
    "    if mask.size(1) > num_words:\n",
    "        mask = mask[:, :num_words]\n",
    "\n",
    "    fake_imgs, attention_maps, _, _ = G(noise, sent_emb, words_embs, mask)\n",
    "    assert len(attention_maps) >= 1\n",
    "    img = fake_imgs[len(attention_maps)].detach().cpu()\n",
    "    lr_img = fake_imgs[len(attention_maps) - 1].detach().cpu()\n",
    "    attn_maps = attention_maps[-1]\n",
    "    att_sze = attn_maps.size(2)\n",
    "    \n",
    "img_set, _ = utils.build_super_images(img, cap, dataset.ixtoword, attn_maps, att_sze,\n",
    "                                      batch_size, cfg['words_num'], lr_imgs=lr_img, nvis=1)\n",
    "\n",
    "print('given caption:', caption)\n",
    "plt.figure()\n",
    "show_tensor(fake_imgs[-1])\n",
    "\n",
    "plt.figure(figsize=(10,10),dpi=500)\n",
    "show_np_arr(img_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
